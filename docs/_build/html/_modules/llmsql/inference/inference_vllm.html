<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>llmsql.inference.inference_vllm &#8212; LLMSQL 0.1.11 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=5349f25f" />
    <link rel="stylesheet" type="text/css" href="../../../_static/basic.css?v=5c69cfe2" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/front_page.css?v=9e26f69c" />
    <script src="../../../_static/documentation_options.js?v=1e825a29"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../../../_static/scripts/front_page.js?v=a59558f4"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">LLMSQL 0.1.11 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">llmsql.inference.inference_vllm</a></li>
      </ul>
    </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">

  <h1>Source code for llmsql.inference.inference_vllm</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">LLMSQL vLLM Inference Function</span>
<span class="sd">==============================</span>

<span class="sd">This module provides a single function `inference_vllm()` that performs</span>
<span class="sd">text-to-SQL generation using large language models via the vLLM backend.</span>

<span class="sd">Example</span>
<span class="sd">-------</span>

<span class="sd">.. code-block:: python</span>

<span class="sd">    from llmsql.inference import inference_vllm</span>

<span class="sd">    results = inference_vllm(</span>
<span class="sd">        model_name=&quot;Qwen/Qwen2.5-1.5B-Instruct&quot;,</span>
<span class="sd">        output_file=&quot;outputs/predictions.jsonl&quot;,</span>
<span class="sd">        questions_path=&quot;data/questions.jsonl&quot;,</span>
<span class="sd">        tables_path=&quot;data/tables.jsonl&quot;,</span>
<span class="sd">        num_fewshots=5,</span>
<span class="sd">        batch_size=8,</span>
<span class="sd">        max_new_tokens=256,</span>
<span class="sd">        temperature=0.7,</span>
<span class="sd">        tensor_parallel_size=1,</span>
<span class="sd">    )</span>

<span class="sd">Notes</span>
<span class="sd">~~~~~</span>

<span class="sd">This function uses the vLLM backend. Outputs may differ from the Transformers</span>
<span class="sd">backend due to differences in implementation, batching, and numerical precision.</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">llmsql.config.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">DEFAULT_WORKDIR_PATH</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">llmsql.loggers.logging_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">log</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">llmsql.utils.inference_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_maybe_download</span><span class="p">,</span> <span class="n">_setup_seed</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">llmsql.utils.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">choose_prompt_builder</span><span class="p">,</span>
    <span class="n">load_jsonl</span><span class="p">,</span>
    <span class="n">overwrite_jsonl</span><span class="p">,</span>
    <span class="n">save_jsonl_lines</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">load_dotenv</span><span class="p">()</span>


<div class="viewcode-block" id="inference_vllm">
<a class="viewcode-back" href="../../../docs/api/inference.html#llmsql.inference.inference_vllm.inference_vllm">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">inference_vllm</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">output_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">questions_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tables_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hf_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tensor_parallel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span>
    <span class="n">workdir_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DEFAULT_WORKDIR_PATH</span><span class="p">,</span>
    <span class="n">num_fewshots</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">llm_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run SQL generation using vLLM.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_name: Hugging Face model name or path.</span>
<span class="sd">        output_file: Path to write outputs (will be overwritten).</span>
<span class="sd">        questions_path: Path to questions.jsonl (optional, auto-download if missing).</span>
<span class="sd">        tables_path: Path to tables.jsonl (optional, auto-download if missing).</span>
<span class="sd">        hf_token: Hugging Face auth token.</span>
<span class="sd">        tensor_parallel_size: Degree of tensor parallelism (for multi-GPU).</span>
<span class="sd">        seed: Random seed.</span>
<span class="sd">        workdir_path: Directory to store any downloaded data.</span>
<span class="sd">        num_fewshots: Number of examples per prompt (0, 1, or 5).</span>
<span class="sd">        batch_size: Number of questions per generation batch.</span>
<span class="sd">        max_new_tokens: Max tokens to generate.</span>
<span class="sd">        temperature: Sampling temperature.</span>
<span class="sd">        do_sample: Whether to sample or use greedy decoding.</span>
<span class="sd">        **llm_kwargs: Extra kwargs forwarded to vllm.LLM().</span>

<span class="sd">    Returns:</span>
<span class="sd">        List of dicts containing `question_id` and generated `completion`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># --- setup ---</span>
    <span class="n">_setup_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">hf_token</span> <span class="o">=</span> <span class="n">hf_token</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;HF_TOKEN&quot;</span><span class="p">)</span>
    <span class="n">workdir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">workdir_path</span><span class="p">)</span>
    <span class="n">workdir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># --- load input data ---</span>
    <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Preparing questions and tables...&quot;</span><span class="p">)</span>
    <span class="n">questions_path</span> <span class="o">=</span> <span class="n">_maybe_download</span><span class="p">(</span><span class="s2">&quot;questions.jsonl&quot;</span><span class="p">,</span> <span class="n">questions_path</span><span class="p">)</span>
    <span class="n">tables_path</span> <span class="o">=</span> <span class="n">_maybe_download</span><span class="p">(</span><span class="s2">&quot;tables.jsonl&quot;</span><span class="p">,</span> <span class="n">tables_path</span><span class="p">)</span>
    <span class="n">questions</span> <span class="o">=</span> <span class="n">load_jsonl</span><span class="p">(</span><span class="n">questions_path</span><span class="p">)</span>
    <span class="n">tables_list</span> <span class="o">=</span> <span class="n">load_jsonl</span><span class="p">(</span><span class="n">tables_path</span><span class="p">)</span>
    <span class="n">tables</span> <span class="o">=</span> <span class="p">{</span><span class="n">t</span><span class="p">[</span><span class="s2">&quot;table_id&quot;</span><span class="p">]:</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tables_list</span><span class="p">}</span>

    <span class="c1"># --- init model ---</span>
    <span class="n">llm_kwargs</span> <span class="o">=</span> <span class="n">llm_kwargs</span> <span class="ow">or</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="s2">&quot;tensor_parallel_size&quot;</span> <span class="ow">in</span> <span class="n">llm_kwargs</span><span class="p">:</span>
        <span class="n">tensor_parallel_size</span> <span class="o">=</span> <span class="n">llm_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tensor_parallel_size&quot;</span><span class="p">)</span>

    <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading vLLM model &#39;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&#39; (tp=</span><span class="si">{</span><span class="n">tensor_parallel_size</span><span class="si">}</span><span class="s2">)...&quot;</span><span class="p">)</span>

    <span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="n">tensor_parallel_size</span><span class="p">,</span>
        <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">llm_kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># --- prepare output file ---</span>
    <span class="n">overwrite_jsonl</span><span class="p">(</span><span class="n">output_file</span><span class="p">)</span>
    <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output will be written to </span><span class="si">{</span><span class="n">output_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># --- prompt builder and sampling params ---</span>
    <span class="n">prompt_builder</span> <span class="o">=</span> <span class="n">choose_prompt_builder</span><span class="p">(</span><span class="n">num_fewshots</span><span class="p">)</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">do_sample</span> <span class="k">else</span> <span class="n">temperature</span>
    <span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># --- main inference loop ---</span>
    <span class="n">all_results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">batch_start</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Generating&quot;</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">questions</span><span class="p">[</span><span class="n">batch_start</span> <span class="p">:</span> <span class="n">batch_start</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>

        <span class="n">prompts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
            <span class="n">tbl</span> <span class="o">=</span> <span class="n">tables</span><span class="p">[</span><span class="n">q</span><span class="p">[</span><span class="s2">&quot;table_id&quot;</span><span class="p">]]</span>
            <span class="n">example_row</span> <span class="o">=</span> <span class="n">tbl</span><span class="p">[</span><span class="s2">&quot;rows&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">tbl</span><span class="p">[</span><span class="s2">&quot;rows&quot;</span><span class="p">]</span> <span class="k">else</span> <span class="p">[]</span>
            <span class="n">prompts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">prompt_builder</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">],</span> <span class="n">tbl</span><span class="p">[</span><span class="s2">&quot;header&quot;</span><span class="p">],</span> <span class="n">tbl</span><span class="p">[</span><span class="s2">&quot;types&quot;</span><span class="p">],</span> <span class="n">example_row</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>

        <span class="n">batch_results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">out</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
            <span class="n">batch_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;question_id&quot;</span><span class="p">:</span> <span class="n">q</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;question_id&quot;</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)),</span>
                    <span class="s2">&quot;completion&quot;</span><span class="p">:</span> <span class="n">text</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>

        <span class="n">save_jsonl_lines</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="n">batch_results</span><span class="p">)</span>
        <span class="n">all_results</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">batch_results</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Saved batch </span><span class="si">{</span><span class="n">batch_start</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_results</span><span class="p">)</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">total</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generation completed. </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_results</span><span class="p">)</span><span class="si">}</span><span class="s2"> results saved to </span><span class="si">{</span><span class="n">output_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">all_results</span></div>

</pre></div>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">LLMSQL 0.1.11 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">llmsql.inference.inference_vllm</a></li>
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright .
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    </div>
  </body>
</html>
