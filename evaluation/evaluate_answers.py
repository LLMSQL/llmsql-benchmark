"""
Evaluate model-generated SQL predictions against the LLMSQL benchmark.

Usage (example from project root):
    python3 evaluation/evaluate_answers.py \
        --outputs_path outputs/my_model_preds.jsonl \
        --questions_path dataset/questions.jsonl \
        --db_file_path dataset/sqlite_tables.db \
        --save_report evaluation/eval_report.json

This script compares SQL queries predicted by a model with the gold (reference)
queries provided in the benchmark. Both predicted and gold SQL queries are
executed against the provided SQLite database, and their results are compared.

The evaluation computes:
  - Accuracy (#correct / #total)
  - SQL execution errors (invalid queries)
  - NULL-result counts (cases where results are empty/None)
  - Mismatch details (up to N shown in console, all saved in report if requested)

Expected input format for predictions (JSONL):
    {"question_id": "1", "completion": "SELECT name FROM Table WHERE age > 30"}
    {"question_id": "2", "completion": "SELECT COUNT(*) FROM Table"}
    {"question_id": "3", "completion": "Some raw text + SQL: SELECT smth FROM Table"}

Notes:
  - `question_id` must match IDs in `questions.jsonl`.
  - `completion` should contain the modelâ€™s SQL prediction (extra text is allowed;
    the evaluation automatically extracts SQL).
  - Run this script from the **project root** (LLMSQL), not inside `evaluation/`.
"""

import argparse
import json
import sqlite3
from typing import Dict, List

from rich.progress import track
from utils.evaluation_utils import (
    evaluate_sample,
)
from utils.logging_config import log
from utils.utils import log_mismatch, print_summary


def main(
    db_path: str,
    outputs_path: str,
    questions_path: str,
    save_report: str = None,
    show_mismatches: bool = True,
    max_mismatches: int = 5,
):
    """
    Evaluate predicted SQL queries against a ground truth benchmark.

    This function compares SQL queries generated by a model with gold (ground-truth)
    SQL queries from a dataset. It executes both the gold SQL and the predicted SQL
    queries against a SQLite database and checks whether they return the same results.

    Args:
        db_path (str): Path to the SQLite database file used for executing queries.
        outputs_path (str): Path to the JSONL file with model predictions.
                            Each line must contain a dict with at least `question_id`
                            and the model's SQL output (completion).
        questions_path (str): Path to the JSONL file with benchmark questions,
                              containing `question_id`, `sql`, and `table_id`.
        save_report (str, optional): If provided, saves a detailed JSON report
                                     with metrics and mismatches to this path.
        show_mismatches (bool, optional): Whether to print details of mismatched queries
                                          during evaluation. Defaults to True.
        max_mismatches (int, optional): Maximum number of mismatches to display in logs.
                                        Defaults to 5.

    Output:
        Prints an evaluation summary (accuracy, SQL errors, etc.) to stdout
        and optionally saves a JSON report with detailed results.
    """
    log.info(f"DB file: {db_path}")

    # Connect to the SQLite database
    conn = sqlite3.connect(db_path)

    # Load benchmark questions into a dictionary:
    #   {question_id: {"sql": ..., "table_id": ..., "question": ...}, ...}
    with open(questions_path, encoding="utf-8") as f:
        questions = {q["question_id"]: q for q in map(json.loads, f)}

    # Load model predictions
    with open(outputs_path, encoding="utf-8") as f:
        outputs = [json.loads(line) for line in f]

    # Metrics dictionary to track evaluation statistics
    metrics = {
        "total": 0,  # total number of evaluated questions
        "matches": 0,  # number of correct predictions
        "pred_none": 0,  # number of predictions returning NULL results
        "gold_none": 0,  # number of gold queries returning NULL results
        "sql_errors": 0,  # number of failed SQL executions
    }

    mismatches: List[Dict] = []  # store details of mismatched predictions

    # Iterate over each prediction with a progress bar
    for item in track(outputs, description="Evaluating"):
        metrics["total"] += 1

        # Evaluate a single prediction
        is_match, mismatch_info, m_update = evaluate_sample(item, questions, conn)

        # Update metrics
        metrics["matches"] += is_match
        metrics["pred_none"] += m_update["pred_none"]
        metrics["gold_none"] += m_update["gold_none"]
        metrics["sql_errors"] += m_update["sql_error"]

        # Log mismatches if requested
        if mismatch_info:
            mismatches.append(mismatch_info)
            if show_mismatches and len(mismatches) <= max_mismatches:
                log_mismatch(**mismatch_info)

    # Print a summary table of results
    print_summary(
        metrics["total"],
        metrics["matches"],
        metrics["pred_none"],
        metrics["gold_none"],
        metrics["sql_errors"],
    )

    # Optionally save full JSON report
    if save_report:
        report = {
            **metrics,
            "accuracy": metrics["matches"] / metrics["total"],
            "mismatches": mismatches,
        }
        with open(save_report, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        log.info(f"Saved report to {save_report}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Evaluate SQL query predictions against a benchmark dataset."
    )
    parser.add_argument(
        "--db_file_path",
        default="dataset/sqlite_tables.db",
        help="Path to the SQLite .db file. "
        "The database can be downloaded using `download_db.py` "
        "(see `evaluation/README.md`).",
    )
    parser.add_argument(
        "--outputs_path",
        required=True,
        help="Path to the JSONL file with the model's predictions.",
    )
    parser.add_argument(
        "--questions_path",
        default="dataset/questions.jsonl",
        help="Path to the benchmark questions JSONL file.",
    )
    parser.add_argument(
        "--save_report",
        help="If provided, save a detailed JSON report with metrics and mismatches.",
    )
    parser.add_argument(
        "--no_mismatches",
        action="store_true",
        help="Do not print details of mismatched predictions during evaluation.",
    )
    parser.add_argument(
        "--max_mismatches",
        type=int,
        default=5,
        help="Maximum number of mismatches to display in logs.",
    )
    args = parser.parse_args()

    main(
        args.db_file_path,
        args.outputs_path,
        args.questions_path,
        save_report=args.save_report,
        show_mismatches=not args.no_mismatches,
        max_mismatches=args.max_mismatches,
    )
